{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature Data Preprocessing\n",
        "\n",
        "1. **Data Extraction**  \n",
        "   - Extract hourly air temperature data from the DWD API.  \n",
        "   - Calculate the mean temperature across all measurement stations.  \n",
        "   - Round the result to two decimal places.\n",
        "\n",
        "2. **Upsampling & Interpolation**  \n",
        "   - Upsample the hourly temperature series to a 15-minute frequency.  \n",
        "   - Apply **Inverse Distance Weighted (IDW) linear interpolation** to fill intermediate timestamps.\n",
        "\n",
        "3. **Forecast Alignment**  \n",
        "   - Shift the entire temperature series by +1 day to simulate forecasted temperature values.  \n",
        "   - Merge with the price dataset on the `delivery_time` column.\n",
        "\n",
        "4. **Standardization**  \n",
        "   - Apply **Z-score standardization** to the temperature feature.\n"
      ],
      "metadata": {
        "id": "VndLALGfkaw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Temperature Data Preprocessing\n",
        "\n",
        "**1.Data Extraction**\n",
        "\n",
        "Hourly air temperature observations were retrieved from the German Weather Service (DWD) Climate Data Center open data API.\n",
        "\n",
        "For each timestamp, the mean temperature across all available stations was calculated.\n",
        "\n",
        "The aggregated values were rounded to two decimal places to ensure consistency.\n",
        "\n",
        "**2.Upsampling & Interpolation**\n",
        "\n",
        "The hourly mean temperature series was upsampled to a 15-minute frequency to match the resolution of the electricity price data.\n",
        "\n",
        "Missing intermediate timestamps were filled using **Inverse Distance Weighted (IDW) linear interpolation**, where weights decrease proportionally with the temporal distance. The interpolation ensures a smooth transition between hourly observations while preserving local variability.\n",
        "\n",
        "**3.Forecast Alignment**\n",
        "\n",
        "To simulate the availability of weather forecasts, the temperature series was shifted by +1 day.\n",
        "\n",
        "The resulting series was aligned with the electricity price dataset through the delivery_time timestamp column.\n",
        "\n",
        "**4.Reproducibility**\n",
        "\n",
        "All preprocessing steps were implemented in Python 3.12 using pandas (≥2.2), numpy (≥1.26), and scipy (≥1.13).\n",
        "\n",
        "The IDW interpolation was implemented manually without reliance on geospatial libraries, to ensure transparency and control over the weighting scheme."
      ],
      "metadata": {
        "id": "_fuoDVbQlzgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPTBxmWtkJC6"
      },
      "outputs": [],
      "source": [
        "import re, io, zipfile\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import pandas as pd\n",
        "\n",
        "class DWDTemperatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Base URLs for historical and recent hourly air temperature datasets provided by DWD (Climate Data Center)\n",
        "        self.historical_url = (\"https://opendata.dwd.de/climate_environment/CDC/\"\n",
        "                               \"observations_germany/climate/hourly/air_temperature/historical/\")\n",
        "        self.recent_url = (\"https://opendata.dwd.de/climate_environment/CDC/\"\n",
        "                           \"observations_germany/climate/hourly/air_temperature/recent/\")\n",
        "        # Configure HTTP session with retry policy and user agent header\n",
        "        self.session = requests.Session()\n",
        "        retry = Retry(total=5, backoff_factor=0.4,\n",
        "                      status_forcelist=[429, 500, 502, 503, 504],\n",
        "                      allowed_methods=[\"GET\"])\n",
        "        self.session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "        self.headers = {\"User-Agent\": \"DWDTemperatureExtractor/1.0\"}\n",
        "\n",
        "    def get_file_list(self, base_url, start_date: datetime, end_date: datetime):\n",
        "        \"\"\"\n",
        "        Retrieve a list of candidate ZIP archives from the given DWD directory.\n",
        "        - For the 'historical' directory: include files whose covered period [file_start, file_end]\n",
        "          overlaps with the requested [start_date, end_date].\n",
        "        - For the 'recent' directory: include all files matching '*_akt.zip'.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            r = self.session.get(base_url, headers=self.headers, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            html = r.text\n",
        "\n",
        "            # Extract all links ending with '.zip'\n",
        "            zip_names = re.findall(r'href=\"([^\"]+\\.zip)\"', html)\n",
        "            out = []\n",
        "\n",
        "            is_hist = \"historical\" in base_url\n",
        "            if is_hist:\n",
        "                # Historical filenames: e.g., stundenwerte_TU_00003_19500401_20110331_hist.zip\n",
        "                pat = re.compile(r\"stundenwerte_TU_(\\d{5})_(\\d{8})_(\\d{8})_hist\\.zip\")\n",
        "                for name in zip_names:\n",
        "                    m = pat.search(name)\n",
        "                    if not m:\n",
        "                        continue\n",
        "                    _, start_s, end_s = m.groups()\n",
        "                    fs = datetime.strptime(start_s, \"%Y%m%d\")\n",
        "                    fe = datetime.strptime(end_s, \"%Y%m%d\")\n",
        "                    # Keep if file coverage overlaps requested range\n",
        "                    if (fs <= end_date) and (fe >= start_date):\n",
        "                        out.append(urljoin(base_url, name))\n",
        "            else:\n",
        "                # Recent filenames: e.g., stundenwerte_TU_<station>_akt.zip\n",
        "                pat = re.compile(r\"stundenwerte_TU_\\d{5}_akt\\.zip\")\n",
        "                for name in zip_names:\n",
        "                    if pat.search(name):\n",
        "                        out.append(urljoin(base_url, name))\n",
        "\n",
        "            return sorted(set(out))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to retrieve file list: {e}\")\n",
        "            return []\n",
        "\n",
        "    def download_and_extract_zip(self, zip_url):\n",
        "        \"\"\"\n",
        "        Download a ZIP archive and return the content of the first file starting with 'produkt_' as text.\n",
        "        DWD files are semicolon-separated text files, typically encoded in UTF-8.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resp = self.session.get(zip_url, headers=self.headers, timeout=60, stream=True)\n",
        "            resp.raise_for_status()\n",
        "            with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
        "                # Select the first 'produkt_*' file (usually containing hourly observations)\n",
        "                data_files = [f for f in zf.namelist() if f.startswith(\"produkt_\")]\n",
        "                if not data_files:\n",
        "                    print(f\"No product file found in {zip_url}\")\n",
        "                    return None\n",
        "                with zf.open(data_files[0]) as f:\n",
        "                    return f.read().decode(\"utf-8\", errors=\"replace\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {zip_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def parse_dwd_data(self, content):\n",
        "        \"\"\"Parse a DWD 'produkt_*' CSV text file into a pandas DataFrame.\"\"\"\n",
        "        if not content:\n",
        "            return None\n",
        "        try:\n",
        "            df = pd.read_csv(io.StringIO(content), sep=';', dtype=str)\n",
        "            df.columns = [c.strip() for c in df.columns]\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Parsing failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def filter_by_date_range(self, df, start_date: datetime, end_date: datetime):\n",
        "        \"\"\"Filter rows by the inclusive range [start_date, end_date] using column MESS_DATUM (format YYYYMMDDHH).\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "        if 'MESS_DATUM' not in df.columns:\n",
        "            print(\"Column 'MESS_DATUM' not found\")\n",
        "            return None\n",
        "        try:\n",
        "            df = df.copy()\n",
        "            df['datetime'] = pd.to_datetime(df['MESS_DATUM'], format='%Y%m%d%H', errors='coerce')\n",
        "            df = df.dropna(subset=['datetime'])\n",
        "            mask = (df['datetime'] >= start_date) & (df['datetime'] <= end_date)\n",
        "            return df.loc[mask]\n",
        "        except Exception as e:\n",
        "            print(f\"Date conversion error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_temperature_data(self, start_date_str: str, end_date_str: str):\n",
        "        \"\"\"\n",
        "        Retrieve and merge all available temperature records within the specified date range.\n",
        "        Both historical and recent archives are considered. The final dataset is de-duplicated\n",
        "        and sorted by station ID and timestamp.\n",
        "        \"\"\"\n",
        "        start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\") + timedelta(hours=23, minutes=59)\n",
        "        print(f\"Extracting date range: {start_date} to {end_date}\")\n",
        "\n",
        "        all_dfs = []\n",
        "\n",
        "        # Process historical archives\n",
        "        print(\"Processing historical data...\")\n",
        "        for zip_url in self.get_file_list(self.historical_url, start_date, end_date):\n",
        "            content = self.download_and_extract_zip(zip_url)\n",
        "            df = self.parse_dwd_data(content)\n",
        "            fdf = self.filter_by_date_range(df, start_date, end_date)\n",
        "            if fdf is not None and not fdf.empty:\n",
        "                all_dfs.append(fdf)\n",
        "\n",
        "        # Process recent archives\n",
        "        print(\"Processing recent data...\")\n",
        "        for zip_url in self.get_file_list(self.recent_url, start_date, end_date):\n",
        "            content = self.download_and_extract_zip(zip_url)\n",
        "            df = self.parse_dwd_data(content)\n",
        "            fdf = self.filter_by_date_range(df, start_date, end_date)\n",
        "            if fdf is not None and not fdf.empty:\n",
        "                all_dfs.append(fdf)\n",
        "\n",
        "        if not all_dfs:\n",
        "            print(\"No records found within the specified range\")\n",
        "            return None\n",
        "\n",
        "        combined = pd.concat(all_dfs, ignore_index=True)\n",
        "        combined = combined.drop_duplicates(subset=['STATIONS_ID', 'MESS_DATUM'])\n",
        "        combined = combined.sort_values(['STATIONS_ID', 'datetime']).reset_index(drop=True)\n",
        "        print(f\"Total records retrieved: {len(combined)}\")\n",
        "        return combined, start_date, end_date\n",
        "\n",
        "    def transform_to_hourly_average(self, df, start_date_str: str, end_date_str: str):\n",
        "        \"\"\"\n",
        "        Aggregate station-level observations to hourly averages across all available stations.\n",
        "        The output includes average temperature, number of stations reporting, and standard deviation.\n",
        "        \"\"\"\n",
        "        if df is None or df.empty:\n",
        "            print(\"Input dataset is empty\")\n",
        "            return None\n",
        "        start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\") + timedelta(hours=23, minutes=59)\n",
        "\n",
        "        # Identify a valid temperature column\n",
        "        temp_col = next((c for c in ['TT_TU', 'LUFTTEMPERATUR', 'TEMPERATURE'] if c in df.columns), None)\n",
        "        if temp_col is None:\n",
        "            print(f\"No temperature column found. Available columns: {list(df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            work = df.copy()\n",
        "            work[temp_col] = pd.to_numeric(work[temp_col], errors='coerce')\n",
        "            # Exclude missing values (DWD convention: -999)\n",
        "            work = work[work[temp_col] > -900]\n",
        "\n",
        "            work['datetime_hour'] = work['datetime'].dt.floor('H')\n",
        "            agg = (work.groupby('datetime_hour')[temp_col]\n",
        "                        .agg(['mean', 'count', 'std'])\n",
        "                        .rename(columns={'mean': 'temperature_avg',\n",
        "                                         'count': 'station_count',\n",
        "                                         'std': 'temperature_std'})\n",
        "                        .reset_index()\n",
        "                        .rename(columns={'datetime_hour': 'datetime'}))\n",
        "\n",
        "            result = agg[['datetime', 'temperature_avg', 'station_count', 'temperature_std']].sort_values('datetime')\n",
        "            print(\"Transformation complete:\")\n",
        "            print(f\"Raw records: {len(df)}, after cleaning: {len(work)}, hourly aggregates: {len(result)}\")\n",
        "            print(f\"Time span: {result['datetime'].min()} → {result['datetime'].max()}\")\n",
        "            print(f\"Average number of reporting stations per hour: {agg['station_count'].mean():.1f}\")\n",
        "            return result, start_date, end_date\n",
        "        except Exception as e:\n",
        "            print(f\"Data transformation error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def save_csv_with_start_year(df: pd.DataFrame, start_date: datetime, prefix: str):\n",
        "        \"\"\"\n",
        "        Save the DataFrame to a CSV file named <prefix>_<YYYY>.csv, where <YYYY> corresponds\n",
        "        to the year of the start_date. This ensures platform-independent file naming.\n",
        "        \"\"\"\n",
        "        year = start_date.strftime('%Y')\n",
        "        fname = f\"{prefix}_{year}.csv\"\n",
        "        df.to_csv(fname, index=False)\n",
        "        print(f\"Saved: {fname}\")\n",
        "        return fname\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Sample Usage of DWDTemperatureExtractor ===\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = DWDTemperatureExtractor()\n",
        "\n",
        "# Define date range (January 2023)\n",
        "start_date = \"2023-01-01\"\n",
        "end_date   = \"2023-01-31\"\n",
        "\n",
        "# Step 1: Extract raw station-level temperature data\n",
        "res = extractor.extract_temperature_data(start_date, end_date)\n",
        "if res is None:\n",
        "    raise RuntimeError(\"No data retrieved for the specified period.\")\n",
        "df_raw, start_dt, end_dt = res\n",
        "\n",
        "# Step 2: Aggregate to hourly averages across stations\n",
        "res2 = extractor.transform_to_hourly_average(df_raw, start_date, end_date)\n",
        "if res2 is None:\n",
        "    raise RuntimeError(\"Hourly aggregation failed.\")\n",
        "hourly_df, _, _ = res2\n",
        "\n",
        "# Step 3: Save results as CSV (prefix 'temperature_hourly')\n",
        "extractor.save_csv_with_start_year(hourly_df, start_dt, prefix=\"temperature_hourly\")\n",
        "\n",
        "print(hourly_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsSYB69pkXMy",
        "outputId": "8ee355c1-1338-4754-9059-f4d885460f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting date range: 2023-01-01 00:00:00 to 2023-01-31 23:59:00\n",
            "Processing historical data...\n",
            "Processing recent data...\n",
            "Total records retrieved: 366656\n",
            "Transformation complete:\n",
            "Raw records: 366656, after cleaning: 365984, hourly aggregates: 744\n",
            "Time span: 2023-01-01 00:00:00 → 2023-01-31 23:00:00\n",
            "Average number of reporting stations per hour: 491.9\n",
            "Saved: temperature_hourly_2023.csv\n",
            "             datetime  temperature_avg  station_count  temperature_std\n",
            "0 2023-01-01 00:00:00        12.501224            490         3.430335\n",
            "1 2023-01-01 01:00:00        12.345808            489         3.516412\n",
            "2 2023-01-01 02:00:00        12.224286            490         3.544961\n",
            "3 2023-01-01 03:00:00        12.060000            490         3.492275\n",
            "4 2023-01-01 04:00:00        11.856735            490         3.487102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-845058197.py:178: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  work['datetime_hour'] = work['datetime'].dt.floor('H')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uj67wodTxVYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}